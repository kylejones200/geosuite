{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef749db2",
   "metadata": {},
   "source": [
    "# Change-Point Analysis\n",
    "\n",
    "This notebook demonstrates automated stratigraphic interpretation using GeoSuite's change-point detection functions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Change-point detection is used to automatically identify formation boundaries in well logs. GeoSuite provides:\n",
    "- **PELT algorithm**: Optimal segmentation with penalty tuning\n",
    "- **Bayesian online detection**: Probabilistic change-point detection\n",
    "- **Preprocessing**: Median filtering and baseline removal\n",
    "- **Consensus picking**: Combine multiple methods for robust results\n",
    "\n",
    "This notebook will show you how to:\n",
    "\n",
    "1. Load and preprocess well log data\n",
    "2. Apply change-point detection algorithms\n",
    "3. Compare multiple methods\n",
    "4. Generate consensus formation tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa1f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GeoSuite modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from geosuite.data import load_demo_well_logs\n",
    "    preprocess_log,\n",
    "    detect_pelt,\n",
    "    detect_bayesian_online,\n",
    "    compare_methods,\n",
    "    find_consensus,\n",
    "    tune_penalty_to_target_count\n",
    ")\n",
    "\n",
    "print(\"GeoSuite imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0b825",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Well Log Data\n",
    "\n",
    "Load well log data and extract a log curve (e.g., gamma ray) for change-point detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ab653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demo well log data\n",
    "df = load_demo_well_logs()\n",
    "\n",
    "print(f\"Loaded {len(df):,} data points\")\n",
    "print(f\"\\nAvailable columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Find gamma ray or similar log for change-point detection\n",
    "gr_cols = [col for col in df.columns if 'GR' in col.upper() or 'GAMMA' in col.upper()]\n",
    "depth_col = 'depth_m' if 'depth_m' in df.columns else 'DEPTH'\n",
    "\n",
    "if gr_cols:\n",
    "    log_col = gr_cols[0]\n",
    "    print(f\"\\nUsing {log_col} for change-point detection\")\n",
    "else:\n",
    "    # Use first numeric column as fallback\n",
    "    numeric_cols = [col for col in df.columns if col != depth_col and pd.api.types.is_numeric_dtype(df[col])]\n",
    "    log_col = numeric_cols[0] if numeric_cols else df.columns[1]\n",
    "    print(f\"\\nUsing {log_col} for change-point detection\")\n",
    "\n",
    "# Extract log values and depth\n",
    "log_values = df[log_col].values\n",
    "depth = df[depth_col].values if depth_col in df.columns else np.arange(len(df))\n",
    "\n",
    "print(f\"\\nLog range: {log_values.min():.1f} to {log_values.max():.1f}\")\n",
    "print(f\"Depth range: {depth.min():.1f} to {depth.max():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d9c05",
   "metadata": {},
   "source": [
    "## 2. Preprocess Log Data\n",
    "\n",
    "Use `preprocess_log()` to remove spikes and drift while preserving formation boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872322a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the log to remove spikes and drift\n",
    "# Median filter removes spikes while preserving sharp edges\n",
    "# Detrending removes long-wavelength drift while preserving bed-scale contrasts\n",
    "log_processed = preprocess_log(\n",
    "    log_values,\n",
    "    median_window=5,      # Remove spikes (5 samples)\n",
    "    detrend_window=100    # Remove drift (100 samples)\n",
    ")\n",
    "\n",
    "print(f\"Original log: {len(log_values)} samples\")\n",
    "print(f\"Processed log: {len(log_processed)} samples\")\n",
    "print(f\"\\nOriginal std: {np.std(log_values):.2f}\")\n",
    "print(f\"Processed std: {np.std(log_processed):.2f}\")\n",
    "\n",
    "# Visualize preprocessing\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(depth, log_values, 'lightgray', linewidth=0.5, label='Original', alpha=0.5)\n",
    "ax.plot(depth, log_processed, 'black', linewidth=1, label='Processed')\n",
    "ax.set_xlabel('Depth')\n",
    "ax.set_ylabel(log_col)\n",
    "ax.set_title('Log Preprocessing - Spike and Drift Removal')\n",
    "ax.legend()\n",
    "ax.invert_yaxis()  # Depth increases downward\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4c9e4",
   "metadata": {},
   "source": [
    "## 3. Detect Change Points with PELT\n",
    "\n",
    "Use `detect_pelt()` for optimal segmentation. PELT finds the global optimum efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect change points using PELT algorithm\n",
    "try:\n",
    "    # Auto-tune penalty for target pick density\n",
    "    # Target: ~8 picks per 500 ft (adjust based on your stratigraphy)\n",
    "    depth_increment = depth[1] - depth[0] if len(depth) > 1 else 0.5\n",
    "    \n",
    "    penalty = tune_penalty_to_target_count(\n",
    "        log_processed,\n",
    "        target_picks_per_500ft=8,\n",
    "        depth_increment_ft=depth_increment\n",
    "    )\n",
    "    \n",
    "    print(f\"Tuned penalty: {penalty:.2f}\")\n",
    "    \n",
    "    # Detect change points\n",
    "    cp_indices = detect_pelt(log_processed, penalty=penalty)\n",
    "    cp_depths = depth[cp_indices] if len(cp_indices) > 0 else np.array([])\n",
    "    \n",
    "    print(f\"\\nPELT detected {len(cp_indices)} change points\")\n",
    "    print(f\"Formation tops (ft):\")\n",
    "    for i, d in enumerate(cp_depths[:10]):  # Show first 10\n",
    "        print(f\"  {i+1:2d}. {d:7.1f}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"PELT requires ruptures library: {e}\")\n",
    "    print(\"Install with: pip install ruptures\")\n",
    "    cp_indices = np.array([])\n",
    "    cp_depths = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b4b8d",
   "metadata": {},
   "source": [
    "## 4. Bayesian Online Change-Point Detection\n",
    "\n",
    "Use `detect_bayesian_online()` for probabilistic change-point detection with uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff104321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian online change-point detection\n",
    "# This method provides probability estimates for each change point\n",
    "cp_bayes, cp_probs = detect_bayesian_online(\n",
    "    log_processed,\n",
    "    expected_segment_length=50.0,  # Expected length between changes (samples)\n",
    "    threshold=0.5                   # Probability threshold for flagging changes\n",
    ")\n",
    "\n",
    "cp_bayes_depths = depth[cp_bayes] if len(cp_bayes) > 0 else np.array([])\n",
    "\n",
    "print(f\"Bayesian detection found {len(cp_bayes)} change points\")\n",
    "print(f\"Max probability: {cp_probs.max():.3f}\")\n",
    "print(f\"\\nFormation tops (ft):\")\n",
    "for i, d in enumerate(cp_bayes_depths[:10]):\n",
    "    prob = cp_probs[cp_bayes[i]] if i < len(cp_bayes) else 0.0\n",
    "    print(f\"  {i+1:2d}. {d:7.1f} (prob={prob:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdd990",
   "metadata": {},
   "source": [
    "## 5. Compare Multiple Methods\n",
    "\n",
    "Use `compare_methods()` to run multiple detection algorithms and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple change-point detection methods\n",
    "try:\n",
    "    results = compare_methods(\n",
    "        log_processed,\n",
    "        depth,\n",
    "        penalties=None,  # Auto-generate penalty range\n",
    "        bayesian_threshold=0.5,\n",
    "        include_kernel=True  # Include RBF kernel-based PELT\n",
    "    )\n",
    "    \n",
    "    print(f\"Ran {len(results)} detection methods:\")\n",
    "    for method_name, method_result in results.items():\n",
    "        n_points = method_result.get('n_points', 0)\n",
    "        print(f\"  - {method_name}: {n_points} change points\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Method comparison failed: {e}\")\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e524db",
   "metadata": {},
   "source": [
    "## 6. Find Consensus Picks\n",
    "\n",
    "Use `find_consensus()` to combine results from multiple methods for robust formation tops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb86b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find consensus picks from multiple methods\n",
    "if results:\n",
    "    consensus = find_consensus(results, tolerance_ft=5.0)\n",
    "    \n",
    "    print(f\"Consensus: {len(consensus)} formation tops\")\n",
    "    print(f\"\\nConsensus formation tops (ft):\")\n",
    "    for i, top_depth in enumerate(consensus):\n",
    "        print(f\"  {i+1:2d}. {top_depth:7.1f} ft\")\n",
    "    \n",
    "    # Visualize consensus picks\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.plot(depth, log_processed, 'black', linewidth=1, label='Processed Log')\n",
    "    \n",
    "    # Mark consensus picks\n",
    "    for cp_depth in consensus:\n",
    "        ax.axhline(y=cp_depth, color='red', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel(log_col)\n",
    "    ax.set_ylabel('Depth')\n",
    "    ax.set_title(f'Change-Point Detection - {len(consensus)} Consensus Picks')\n",
    "    ax.legend()\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results available for consensus picking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ea0cc",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "-  Preprocessing logs with `preprocess_log()` (spike removal, drift correction)\n",
    "-  PELT change-point detection with `detect_pelt()` and penalty tuning\n",
    "-  Bayesian online detection with `detect_bayesian_online()`\n",
    "-  Comparing multiple methods with `compare_methods()`\n",
    "-  Finding consensus picks with `find_consensus()`\n",
    "\n",
    "### Key Functions Used\n",
    "\n",
    "- `preprocess_log()`: Remove spikes and drift while preserving boundaries\n",
    "- `detect_pelt()`: Optimal segmentation (requires ruptures library)\n",
    "- `detect_bayesian_online()`: Probabilistic detection (Numba-accelerated)\n",
    "- `tune_penalty_to_target_count()`: Auto-tune PELT penalty\n",
    "- `compare_methods()`: Run multiple algorithms\n",
    "- `find_consensus()`: Combine results for robust picks\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Validate picks against core tops or mudlog markers\n",
    "- Adjust penalty/threshold based on formation thickness\n",
    "- Use multiple logs (GR, resistivity, density) for multi-log consensus\n",
    "- Export picks to well database or interpretation software"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}